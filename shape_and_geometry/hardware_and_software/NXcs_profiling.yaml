category: base
doc: |
  Storage for performance/profiling data which an app collects at runtime.
  
  Performance monitoring and benchmarking of software is a task where questions
  can be asked at various levels of detail. In general there are three main
  contributions to performance:
  
  * Hardware capabilities and configuration
  * Software configuration and capabilities
  * Dynamic effects of the system in operation and working together with
    eventually multiple computers, especially when these have to exchange
    information across a network.
  
  At the most basic level users may wish to document how long e.g. a data
  analysis with a scientific software (app).
  A frequent idea is here to judge how critical the effect is on the workflow
  of the scientists, i.e. is the analysis possible in a few seconds or would it
  take days if I were to run this analysis on a comparable machine. In this case,
  mainly the order of magnitude is relevant, as well as how this can be achieved
  with using parallelization (i.e. reporting the number of CPU and GPU resources
  used, the number of processes and/or threads, and basic details about the
  computing node/computer.
  
  At more advanced levels benchmarks may go as deep as detailed temporal tracking
  of individual processor instructions, their relation to other instructions, the
  state of call stacks, in short eventually the entire app execution history
  and hardware state history. Such analyses are mainly used for performance
  optimization as well as for tracking bugs and other development purposes.
  Specialized software exists which documents such performance data in
  specifically-formatted event log files or databases.
  
  This base class cannot and should not replace these specific solutions.
  Instead, the intention of the base class is to serve scientists at the 
  basic level to enable simple monitoring of performance data and log profiling
  data of key algorithmic steps or parts of computational workflows, so that
  these pieces of information can guide users which order of magnitude differences
  should be expected or not.
  
  Developers of application definitions should add additional fields and
  references to e.g. more detailed performance data to which they wish to link
  the metadata in this base class.
(NXcs_profiling):
  # details about queuing systems etc
  command_line_call(NX_CHAR):
    doc: |
      Command line call with arguments if applicable.
  start_time(NX_DATE_TIME):
    doc: |
      ISO 8601 time code with local time zone offset to UTC information
      included when the app was started.
  end_time(NX_DATE_TIME):
    doc: |
      ISO 8601 time code with local time zone offset to UTC information
      included when the app terminated or crashed.
  total_elapsed_time(NX_NUMBER):
    doc: |
      Wall-clock time how long the app execution took. This may be in principle
      end_time minus start_time; however usage of eventually more precise timers
      may warrant to use a finer temporal discretization,
      and thus demand a more precise record of the wall-clock time.
    unit: NX_TIME
  number_of_processes(NX_INT):
    doc: |
      Qualifier which specifies with how many nominal processes the app was 
      invoked. The main idea behind this field, for instance for app using a
      Message Passing Interface parallelization is to communicate how many
      processes were used.
      
      For sequentially running apps number_of_processes and number_of_threads
      is 1. If the app uses exclusively GPU parallelization number_of_gpus
      can be larger than 1. If no GPU is used number_of_gpus is 0 even though
      the hardware may have GPUs installed, thus indicating these were not
      used though.
    unit: NX_UNITLESS
  number_of_threads(NX_INT):
    doc: |
      Qualifier with how many nominal threads were accessible to the app at
      runtime.
    unit: NX_UNITLESS
  number_of_gpus(NX_INT):
    doc: |
      Qualifier with how many nominal GPUs the app was invoked at runtime.
    unit: NX_UNITLESS
  # there are more complicated usage models, where GPUs are activated in
  # between runs etc, but here application definition and base class developers
  # should feel free to suggest customizations wrt to if and how such more
  # complicated models should be captured.
  # how can you have an empty list?
  (NXcs_computer):
    doc: |
      A collection with one or more computing nodes each with own resources.
      This can be as simple as a laptop or the nodes of a cluster computer.
    name:
      doc: Given name/alias to the computing system, e.g. MyDesktop.
    operating_system:
      doc: |
        Name of the operating system, e.g. Windows, Linux, Mac, Android.
      \@version:
        doc: | 
          Version plus build number, commit hash, or description of an ever 
          persistent resource where the source code of the program and build
          instructions can be found so that the program can be configured in
          such a manner that the result file is ideally recreatable yielding
          the same results.
    # difference e.g. in Win11 between hardware ID, UUID, and device ID
    uuid:
      doc: |
        Ideally a (globally) unique persistent identifier of the computer, i.e.
        the Universally Unique Identifier (UUID) of the computing node.
    # when it comes to performance monitoring
    (NXcs_cpu):
      doc: A list of physical processing units (can be multi-core chips).
    (NXcs_gpu):
      doc: A list of physical coprocessor/graphic cards/accelerator units.
    (NXcs_mm_sys):
      doc: Details about the memory sub-system.
    (NXcs_io_sys):
      doc: Details about the I/O sub-system.
  (NXcs_profiling_event):
    doc: |
      A collection of individual profiling event data which detail e.g. how
      much time the app took for certain computational steps and/or how much
      memory was consumed during these operations.
# how to retrieve UUID for cloud computing instances
# https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/identify_ec2_instances.html
